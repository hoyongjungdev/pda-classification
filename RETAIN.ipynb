{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RETAIN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNCwdFmuPSKS5l1POvV17rY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoyongjungdev/pda-classification/blob/main/RETAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev9v0siagFuP"
      },
      "source": [
        "# RETAIN model with GRU\n",
        "class RETAIN(nn.Module):\n",
        "  def __init__(self, params:dict):\n",
        "    super(RETAIN, self).__init__()\n",
        "    self.device = params[\"device\"]\n",
        "\n",
        "    # 1. Embedding\n",
        "    self.emb_layer = nn.Linear(in_features=params[\"num_embeddings\"], out_features=params[\"embedding_dim\"])\n",
        "\n",
        "    # 2. visit-level attention\n",
        "    self.visit_level_rnn = nn.GRU(params[\"visit_rnn_hidden_size\"], params[\"visit_rnn_output_size\"]).to(self.device)\n",
        "    self.visit_hidden_size = params[\"visit_rnn_hidden_size\"]\n",
        "    self.visit_level_attention = nn.Linear(params[\"visit_rnn_output_size\"], params[\"visit_attn_output_size\"]) # α (scalar)\n",
        "\n",
        "    # 3. variable-level attention\n",
        "    self.variable_level_rnn = nn.GRU(params[\"var_rnn_hidden_size\"], params[\"var_rnn_output_size\"]).to(self.device)\n",
        "    self.var_hidden_size = params[\"var_rnn_hidden_size\"]\n",
        "    self.variable_level_attention = nn.Linear(params[\"var_rnn_output_size\"], params[\"var_attn_output_size\"]) # β (vector)\n",
        "\n",
        "    # etc\n",
        "    self.dropout = nn.Dropout(params[\"dropout_p\"])\n",
        "    self.output_dropout = nn.Dropout(params[\"output_dropout_p\"])\n",
        "    self.output_layer = nn.Linear(params[\"embedding_output_size\"], params[\"num_class\"])\n",
        "    self.d = 1\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "    # forwarding : get 2 attentions\n",
        "    \n",
        "    # 1. Embedding\n",
        "    v = self.emb_layer(input)\n",
        "    v = self.dropout(v)\n",
        "\n",
        "    # 2. visit-level attention\n",
        "    visit_rnn_hidden = torch.zeros(self.d, input.size()[0], self.visit_hidden_size).to(self.device) # initial\n",
        "    visit_rnn_output, visit_rnn_hidden = self.visit_level_rnn(torch.flip(v, [0]), visit_rnn_hidden) # in reverse order\n",
        "    alpha = self.visit_level_attention(torch.flip(visit_rnn_output, [0]))\n",
        "    visit_attn_w = F.softmax(alpha, dim=0)\n",
        "\n",
        "    # 3. variable-level attention\n",
        "    var_rnn_hidden = torch.zeros(self.d, input.size()[0], self.var_hidden_size).to(self.device) # initial\n",
        "    var_rnn_output, var_rnn_hidden = self.variable_level_rnn(torch.flip(v, [0]), var_rnn_hidden) # in reverse order\n",
        "    beta = self.variable_level_attention(torch.flip(var_rnn_output, [0]))\n",
        "    var_attn_w = torch.tanh(beta)\n",
        "\n",
        "    # 4. generate context vector\n",
        "    attn_w = visit_attn_w * var_attn_w\n",
        "    c = torch.sum(attn_w * v, dim=0)\n",
        "    c = self.output_dropout(c)\n",
        "\n",
        "    # 5. prediction\n",
        "    output = self.output_layer(c)\n",
        "    output = F.softmax(output, dim=1)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuKn8PUJqebe"
      },
      "source": [
        "# parameters\n",
        "def init_params(params: dict):\n",
        "    # embedding matrix\n",
        "    params[\"num_embeddings\"] = 28 # input dimension\n",
        "    params[\"embedding_dim\"] = 128\n",
        "\n",
        "    # embedding dropout\n",
        "    params[\"dropout_p\"] = 0.5\n",
        "\n",
        "    # Alpha (scalar)\n",
        "    params[\"visit_rnn_hidden_size\"] = 128\n",
        "    params[\"visit_rnn_output_size\"] = 128\n",
        "    params[\"visit_attn_output_size\"] = 1\n",
        "    # Beta (vector)\n",
        "    params[\"var_rnn_hidden_size\"] = 128\n",
        "    params[\"var_rnn_output_size\"] = 128\n",
        "    params[\"var_attn_output_size\"] = 128\n",
        "\n",
        "    params[\"embedding_output_size\"] = 128\n",
        "    params[\"num_class\"] = 2 # 0 or 1\n",
        "    params[\"output_dropout_p\"] = 0.8\n",
        "    params[\"device\"] = device"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}